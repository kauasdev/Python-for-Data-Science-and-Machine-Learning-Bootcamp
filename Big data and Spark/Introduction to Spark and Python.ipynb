{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cb99fe",
   "metadata": {},
   "source": [
    "# Introdução ao Spark e Python\n",
    "\n",
    "Vamos aprender como usar o Spark com Python usando a biblioteca pyspark! Certifique-se de assistir à palestra em vídeo que explica o Spark e RDDs antes de continuar com este código.\n",
    "\n",
    "Este notebook servirá como código de referência para a seção de Big Data do curso envolvendo a Amazon Web Services. O vídeo fornecerá explicações mais detalhadas sobre o que o código está fazendo.\n",
    "\n",
    "## Criando um SparkContext\n",
    "\n",
    "Primeiro, precisamos criar um SparkContext. Vamos importá-lo do pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2652307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862add0",
   "metadata": {},
   "source": [
    "Agora crie o SparkContext. Um SparkContext representa a conexão com um cluster Spark e pode ser usado para criar um RDD e transmitir variáveis nesse cluster.\n",
    "\n",
    "*Nota! Você só pode ter um SparkContext por vez da maneira como estamos executando as coisas aqui.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86219280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/04 09:50:15 WARN Utils: Your hostname, kauas-ThinkPad-T480 resolves to a loopback address: 127.0.1.1; using 192.168.43.253 instead (on interface wlp3s0)\n",
      "23/05/04 09:50:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/04 09:50:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030de13",
   "metadata": {},
   "source": [
    "## Operações Básicas\n",
    "\n",
    "Vamos começar com um exemplo simples, que é ler um arquivo de texto. Primeiro, vamos criar um arquivo de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89866c",
   "metadata": {},
   "source": [
    "Vamos escrever um exemplo de arquivo de texto para ler. Usaremos alguns comandos especiais do Jupyter Notebook para fazer isso, mas sinta-se à vontade para usar qualquer arquivo .txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef76179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "fourth line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ad48e",
   "metadata": {},
   "source": [
    "### Criando o RDD\n",
    "\n",
    "Agora podemos ler o arquivo de texto usando o método **textFile** do SparkContext que criamos. Esse método lerá um arquivo de texto do HDFS, do sistema de arquivos local (disponível em todos os nós) ou de qualquer URI de sistema de arquivos suportado pelo Hadoop e o retornará como um RDD de Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e758cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile('example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18d6c2",
   "metadata": {},
   "source": [
    "A principal abstração do Spark é uma coleção distribuída de itens chamada Resilient Distributed Dataset (RDD). Os RDDs podem ser criados a partir de InputFormats do Hadoop (como arquivos HDFS) ou por meio da transformação de outros RDDs.\n",
    "\n",
    "### Ações\n",
    "\n",
    "Acabamos de criar um RDD usando o método textFile e podemos realizar operações nesse objeto, como contar as linhas.\n",
    "\n",
    "Os RDDs possuem ações, que retornam valores, e transformações, que retornam ponteiros para novos RDDs. Vamos começar com algumas ações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec147712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546aeb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first line'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc060ed8",
   "metadata": {},
   "source": [
    "### Transformações\n",
    "\n",
    "Agora podemos usar as transformações. Por exemplo, a transformação filter retornará um novo RDD com um subconjunto de itens do arquivo. Vamos criar uma transformação de exemplo usando o método filter(). Esse método (assim como a função filter própria do Python) retornará apenas elementos que satisfazem a condição. Vamos tentar procurar por linhas que contenham a palavra 'segundo'. Nesse caso, deverá haver apenas uma linha que contenha essa palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b3de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "secfind = textFile.filter(lambda line: 'second' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef0db59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secfind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760edc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['second line']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secfind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6900379c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secfind.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55504599",
   "metadata": {},
   "source": [
    "Observe que as transformações não exibirão uma saída e não serão executadas até que uma ação seja chamada. Na próxima palestra: Spark Avançado e Python, veremos muitos mais exemplos dessa relação entre transformação e ação!\n",
    "\n",
    "# Ótimo trabalho!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
